{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Import data\n"],"metadata":{"id":"wkFoBl7tBhHS"}},{"cell_type":"code","source":["# Import data\n","from google.colab import drive\n","import pandas as pd\n","import numpy as np\n","import gc\n","import torch\n","\n","# Import preprocessed data\n","drive.mount('/content/drive', force_remount=True)\n","file_path_trd = '/content/drive/MyDrive/Classes/Intro ML/Project/Dataset/train_data_2.csv'\n","file_path_trl = '/content/drive/MyDrive/Classes/Intro ML/Project/Dataset/train_labels_2.csv'\n","file_path_ted = '/content/drive/MyDrive/Classes/Intro ML/Project/Dataset/test_data_2.csv'\n","file_path_tel = '/content/drive/MyDrive/Classes/Intro ML/Project/Dataset/test_labels_2.csv'\n","\n","train_data = pd.read_csv(file_path_trd).to_numpy()\n","train_labels = pd.read_csv(file_path_trl).to_numpy()\n","test_data = pd.read_csv(file_path_ted).to_numpy()\n","test_labels = pd.read_csv(file_path_tel).to_numpy()\n","\n","# Convert to pytorch tensors\n","train_data = torch.tensor(train_data, dtype=torch.float32)\n","train_labels = torch.tensor(train_labels, dtype=torch.long) # Use long for class indices\n","test_data = torch.tensor(test_data, dtype=torch.float32)\n","test_labels = torch.tensor(test_labels, dtype=torch.long) # Use long for class indices"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"xyzyPf6Qbvga","executionInfo":{"status":"error","timestamp":1765237339898,"user_tz":300,"elapsed":40589,"user":{"displayName":"Brandon Williams","userId":"00870841246235053693"}},"outputId":"041aa7ec-1f5b-4d01-cafc-539d4e21f5f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/Classes/Intro ML/Project/Dataset/train_data_2.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3741387526.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mfile_path_tel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Classes/Intro ML/Project/Dataset/test_labels_2.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path_trd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path_trl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path_ted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Classes/Intro ML/Project/Dataset/train_data_2.csv'"]}]},{"cell_type":"markdown","source":["# Linear Model"],"metadata":{"id":"pmZKlHWEvyPZ"}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, recall_score, f1_score\n","\n","# Ensure labels are 1D arrays for scikit-learn\n","train_labels_np = train_labels.cpu().numpy().ravel()\n","test_labels_np = test_labels.cpu().numpy().ravel()\n","\n","# Initialize and train the Logistic Regression classifier\n","# Using 'liblinear' solver which is good for small datasets and L1/L2 regularization\n","# 'multi_class' set to 'auto' for multi-class classification\n","log_reg_classifier = LogisticRegression(random_state=42, solver='liblinear', multi_class='auto')\n","log_reg_classifier.fit(train_data.cpu().numpy(), train_labels_np)\n","\n","# Make predictions on the test data\n","test_predictions = log_reg_classifier.predict(test_data.cpu().numpy())\n","\n","# Evaluate the classifier on test data\n","test_accuracy = accuracy_score(test_labels_np, test_predictions)\n","test_recall = recall_score(test_labels_np, test_predictions, average='weighted') # Use 'weighted' for multi-class\n","test_f1 = f1_score(test_labels_np, test_predictions, average='weighted')         # Use 'weighted' for multi-class\n","\n","print(f\"Logistic Regression Classifier Performance (Test Data):\")\n","print(f\"Accuracy: \\t{test_accuracy:.4f}\")\n","print(f\"Recall: \\t{test_recall:.4f}\")\n","print(f\"F1-Score: \\t{test_f1:.4f}\")\n","\n","# Make predictions on the training data\n","train_predictions = log_reg_classifier.predict(train_data.cpu().numpy())\n","\n","# Evaluate the classifier on training data\n","train_accuracy = accuracy_score(train_labels_np, train_predictions)\n","train_recall = recall_score(train_labels_np, train_predictions, average='weighted') # Use 'weighted' for multi-class\n","train_f1 = f1_score(train_labels_np, train_predictions, average='weighted')         # Use 'weighted' for multi-class\n","\n","print(f\"\\nLogistic Regression Classifier Performance (Train Data):\")\n","print(f\"Accuracy: \\t{train_accuracy:.4f}\")\n","print(f\"Recall: \\t{train_recall:.4f}\")\n","print(f\"F1-Score: \\t{train_f1:.4f}\")"],"metadata":{"id":"UjQeCJYlBgCU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# SVM Classifier"],"metadata":{"id":"frV2aa8mv6lY"}},{"cell_type":"code","source":["from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, recall_score, f1_score\n","\n","# Convert PyTorch tensors to NumPy arrays for scikit-learn\n","train_data_np = train_data.cpu().numpy()\n","train_labels_np = train_labels.cpu().numpy().ravel() # Ensure labels are 1D arrays\n","test_data_np = test_data.cpu().numpy()\n","test_labels_np = test_labels.cpu().numpy().ravel()   # Ensure labels are 1D arrays\n","\n","# Initialize and train the SVM classifier\n","svm_classifier = SVC(random_state=42)\n","svm_classifier.fit(train_data_np, train_labels_np)\n","\n","# Make predictions on the test data\n","test_predictions = svm_classifier.predict(test_data_np)\n","\n","# Evaluate the classifier on test data\n","test_accuracy = accuracy_score(test_labels_np, test_predictions)\n","test_recall = recall_score(test_labels_np, test_predictions, average='weighted') # Use 'weighted' for multi-class\n","test_f1 = f1_score(test_labels_np, test_predictions, average='weighted')         # Use 'weighted' for multi-class\n","\n","print(f\"SVM Classifier Performance (Test Data):\")\n","print(f\"Accuracy: \\t{test_accuracy:.4f}\")\n","print(f\"Recall: \\t{test_recall:.4f}\")\n","print(f\"F1-Score: \\t{test_f1:.4f}\")\n","\n","# Make predictions on the training data\n","train_predictions = svm_classifier.predict(train_data_np)\n","\n","# Evaluate the classifier on training data\n","train_accuracy = accuracy_score(train_labels_np, train_predictions)\n","train_recall = recall_score(train_labels_np, train_predictions, average='weighted') # Use 'weighted' for multi-class\n","train_f1 = f1_score(train_labels_np, train_predictions, average='weighted')         # Use 'weighted' for multi-class\n","\n","print(f\"\\nSVM Classifier Performance (Train Data):\")\n","print(f\"Accuracy: \\t{train_accuracy:.4f}\")\n","print(f\"Recall: \\t{train_recall:.4f}\")\n","print(f\"F1-Score: \\t{train_f1:.4f}\")"],"metadata":{"id":"eJK1_yy4yrdA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# FNN with Weight Decay"],"metadata":{"id":"mtppzeFRvmSH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"D8GIXihRbXq8","collapsed":true},"outputs":[],"source":["import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from sklearn.metrics import accuracy_score, recall_score, f1_score # Import scikit-learn metrics\n","\n","# Get GPU\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(f'Using device: {device}')\n","\n","# Training parameters\n","epochs = 500\n","learning_rate = 2e-2\n","\n","# Model\n","input_features = 4   # Training features\n","output_features = 3                   # Number of actions\n","hidden_layers = [10, 20, 30, 20]\n","class FNN_WD(nn.Module):\n","    def __init__(self):\n","      super(FNN_WD, self).__init__()\n","      self.act = nn.Tanh()\n","      self.fc1 = nn.Linear(input_features, hidden_layers[0])\n","      self.fc2 = nn.Linear(hidden_layers[0], hidden_layers[1])\n","      self.fc3 = nn.Linear(hidden_layers[1], hidden_layers[2])\n","      self.fc4 = nn.Linear(hidden_layers[2], hidden_layers[3])\n","      self.fc5 = nn.Linear(hidden_layers[3], output_features)\n","      self.out = nn.Softmax(dim=1)\n","    def forward(self, x):\n","      x = self.act(self.fc1(x))\n","      x = self.act(self.fc2(x))\n","      x = self.act(self.fc3(x))\n","      x = self.act(self.fc4(x))\n","      x = self.out(self.fc5(x))\n","      return x\n","\n","# H = hit\n","# S = stand\n","# D = double\n","# P = split\n","\n","# Use GPU\n","model = FNN_WD().to(device)\n","train_data = train_data.to(device)\n","train_labels = train_labels.squeeze().to(device) # Added .squeeze()\n","test_data = test_data.to(device)\n","test_labels = test_labels.squeeze().to(device)   # Added .squeeze()\n","\n","# Loss\n","loss_function = nn.CrossEntropyLoss() # This loss function is ideal for our multiple output classifier\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.001)\n","\n","# Initialize lists to store metrics\n","train_accuracies = []\n","train_recalls = []\n","train_f1_scores = []\n","\n","# Train model\n","for epoch in range(epochs):\n","    # Forward pass\n","    outputs = model(train_data)\n","    loss = loss_function(outputs, train_labels)\n","\n","    # Backward and optimize\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    if (epoch + 1) % 1 == 0:\n","      model.eval()\n","      with torch.no_grad():\n","        outputs = model(test_data)\n","        _, predicted = torch.max(outputs.data, 1)\n","\n","        # Convert tensors to numpy arrays for scikit-learn\n","        predicted_np = predicted.cpu().numpy()\n","        test_labels_np = test_labels.cpu().numpy()\n","\n","        # Calculate metrics\n","        accuracy = accuracy_score(test_labels_np, predicted_np)\n","        recall = recall_score(test_labels_np, predicted_np, average='weighted') # Use 'weighted' for multi-class\n","        f1 = f1_score(test_labels_np, predicted_np, average='weighted')       # Use 'weighted' for multi-class\n","\n","        # Store metrics\n","        train_accuracies.append(accuracy)\n","        train_recalls.append(recall)\n","        train_f1_scores.append(f1)\n","\n","        #print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}')\n","\n","import matplotlib.pyplot as plt\n","\n","# Create a list of epoch numbers corresponding to the stored metrics\n","epochs_for_plot = [(i + 1) for i in range(len(train_accuracies))]\n","\n","# Plot the metrics\n","plt.figure(figsize=(10, 6))\n","plt.plot(epochs_for_plot, train_accuracies, label='Accuracy')\n","plt.plot(epochs_for_plot, train_recalls, label='Recall')\n","plt.plot(epochs_for_plot, train_f1_scores, label='F1-Score')\n","\n","plt.xlabel('Epoch')\n","plt.ylabel('Metric Value')\n","plt.title('FNN Training Metrics Over Epochs')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","file_path_model = '/content/drive/MyDrive/Classes/Intro ML/Project/Models/FNN_WD_2.pth'\n","torch.save(model, file_path_model)"]},{"cell_type":"code","source":["print(\"Metrics for FNN with weight decay (Test Data):\")\n","print(\"Accuracy: \\t\", train_accuracies[-1])\n","print(\"Recall: \\t\", train_recalls[-1])\n","print(\"F1: \\t\\t\", train_f1_scores[-1])\n","\n","# Calculate metrics for the training dataset\n","model.eval() # Set the model to evaluation mode\n","with torch.no_grad():\n","    train_outputs = model(train_data)\n","    _, train_predicted = torch.max(train_outputs.data, 1)\n","\n","    # Convert tensors to numpy arrays for scikit-learn\n","    train_predicted_np = train_predicted.cpu().numpy()\n","    train_labels_np = train_labels.cpu().numpy()\n","\n","    # Calculate metrics for training data\n","    train_accuracy = accuracy_score(train_labels_np, train_predicted_np)\n","    train_recall = recall_score(train_labels_np, train_predicted_np, average='weighted')\n","    train_f1 = f1_score(train_labels_np, train_predicted_np, average='weighted')\n","\n","print(\"\\nMetrics for FNN with weight decay (Train Data):\")\n","print(\"Accuracy: \\t\", train_accuracy)\n","print(\"Recall: \\t\", train_recall)\n","print(\"F1: \\t\\t\", train_f1)"],"metadata":{"id":"C_usETJx5Doy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# FNN with Dropout"],"metadata":{"id":"pe9jm4r_wL5q"}},{"cell_type":"code","source":["import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from sklearn.metrics import accuracy_score, recall_score, f1_score # Import scikit-learn metrics\n","\n","# Get GPU\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(f'Using device: {device}')\n","\n","# Training parameters\n","epochs = 300\n","learning_rate = 1e-2\n","\n","# Model\n","input_features = 4   # Training features\n","output_features = 3                   # Number of actions\n","hidden_layers = [10, 20, 30, 20]\n","class FNN_D(nn.Module):\n","    def __init__(self):\n","      super(FNN_D, self).__init__()\n","      self.act = nn.ReLU()\n","      self.fc1 = nn.Linear(input_features, hidden_layers[0])\n","      self.fc2 = nn.Linear(hidden_layers[0], hidden_layers[1])\n","      self.fc3 = nn.Linear(hidden_layers[1], hidden_layers[2])\n","      self.fc4 = nn.Linear(hidden_layers[2], hidden_layers[3])\n","      self.fc5 = nn.Linear(hidden_layers[3], output_features)\n","      self.reg = nn.Dropout(p=0.5)\n","      self.out = nn.Softmax(dim=1)\n","    def forward(self, x):\n","      x = self.act(self.reg(self.fc1(x)))\n","      x = self.act(self.reg(self.fc2(x)))\n","      x = self.act(self.reg(self.fc3(x)))\n","      x = self.act(self.reg(self.fc4(x)))\n","      x = self.out(self.fc5(x))\n","      return x\n","\n","# H = hit\n","# S = stand\n","# D = double\n","\n","# Use GPU\n","model = FNN_D().to(device)\n","train_data = train_data.to(device)\n","train_labels = train_labels.squeeze().to(device)\n","test_data = test_data.to(device)\n","test_labels = test_labels.squeeze().to(device)\n","\n","# Loss\n","loss_function = nn.CrossEntropyLoss() # This loss function is ideal for the multiple output classifier\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Initialize lists to store metrics\n","train_accuracies = []\n","train_recalls = []\n","train_f1_scores = []\n","\n","# Train model\n","for epoch in range(epochs):\n","    # Forward pass\n","    outputs = model(train_data)\n","    loss = loss_function(outputs, train_labels)\n","\n","    # Backward and optimize\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    if (epoch + 1) % 1 == 0:\n","      model.eval()\n","      with torch.no_grad():\n","        outputs = model(test_data)\n","        _, predicted = torch.max(outputs.data, 1)\n","\n","        # Convert tensors to numpy arrays for scikit-learn\n","        predicted_np = predicted.cpu().numpy()\n","        test_labels_np = test_labels.cpu().numpy()\n","\n","        # Calculate metrics\n","        accuracy = accuracy_score(test_labels_np, predicted_np)\n","        recall = recall_score(test_labels_np, predicted_np, average='weighted') # Use 'weighted' for multi-class\n","        f1 = f1_score(test_labels_np, predicted_np, average='weighted')       # Use 'weighted' for multi-class\n","\n","        # Store metrics\n","        train_accuracies.append(accuracy)\n","        train_recalls.append(recall)\n","        train_f1_scores.append(f1)\n","\n","        #print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}')\n","\n","import matplotlib.pyplot as plt\n","\n","# Create a list of epoch numbers corresponding to the stored metrics\n","epochs_for_plot = [(i + 1) for i in range(len(train_accuracies))]\n","\n","# Plot the metrics\n","plt.figure(figsize=(10, 6))\n","plt.plot(epochs_for_plot, train_accuracies, label='Accuracy')\n","plt.plot(epochs_for_plot, train_recalls, label='Recall')\n","plt.plot(epochs_for_plot, train_f1_scores, label='F1-Score')\n","\n","plt.xlabel('Epoch')\n","plt.ylabel('Metric Value')\n","plt.title('FNN Training Metrics Over Epochs')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","file_path_model = '/content/drive/MyDrive/Classes/Intro ML/Project/Models/FNN_D_2.pth'\n","torch.save(model, file_path_model)"],"metadata":{"id":"ObpghrbC9AQ5","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Metrics for FNN with dropout (Test Data):\")\n","print(\"Accuracy: \\t\", train_accuracies[-1])\n","print(\"Recall: \\t\", train_recalls[-1])\n","print(\"F1: \\t\\t\", train_f1_scores[-1])\n","\n","# Calculate metrics for the training dataset\n","model.eval() # Set the model to evaluation mode\n","with torch.no_grad():\n","    train_outputs = model(train_data)\n","    _, train_predicted = torch.max(train_outputs.data, 1)\n","\n","    # Convert tensors to numpy arrays for scikit-learn\n","    train_predicted_np = train_predicted.cpu().numpy()\n","    train_labels_np = train_labels.cpu().numpy()\n","\n","    # Calculate metrics for training data\n","    train_accuracy = accuracy_score(train_labels_np, train_predicted_np)\n","    train_recall = recall_score(train_labels_np, train_predicted_np, average='weighted')\n","    train_f1 = f1_score(train_labels_np, train_predicted_np, average='weighted')\n","\n","print(\"\\nMetrics for FNN with dropout (Train Data):\")\n","print(\"Accuracy: \\t\", train_accuracy)\n","print(\"Recall: \\t\", train_recall)\n","print(\"F1: \\t\\t\", train_f1)"],"metadata":{"id":"goG5qAGm4_hS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# FNN with Batch Normalization"],"metadata":{"id":"g3Sw6lovwPg3"}},{"cell_type":"code","source":["import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from sklearn.metrics import accuracy_score, recall_score, f1_score # Import scikit-learn metrics\n","\n","# Get GPU\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(f'Using device: {device}')\n","\n","# Training parameters\n","epochs = 300\n","learning_rate = 1e-2\n","\n","# Model\n","input_features = 4   # Training features\n","output_features = 3                   # Number of actions\n","hidden_layers = [10, 20, 30, 20]\n","class FNN_BN(nn.Module):\n","    def __init__(self):\n","      super(FNN_BN, self).__init__()\n","      self.act = nn.ReLU()\n","      self.fc1 = nn.Linear(input_features, hidden_layers[0])\n","      self.bn1 = nn.BatchNorm1d(hidden_layers[0])\n","      self.fc2 = nn.Linear(hidden_layers[0], hidden_layers[1])\n","      self.bn2 = nn.BatchNorm1d(hidden_layers[1])\n","      self.fc3 = nn.Linear(hidden_layers[1], hidden_layers[2])\n","      self.bn3 = nn.BatchNorm1d(hidden_layers[2])\n","      self.fc4 = nn.Linear(hidden_layers[2], hidden_layers[3])\n","      self.bn4 = nn.BatchNorm1d(hidden_layers[3])\n","      self.fc5 = nn.Linear(hidden_layers[3], output_features)\n","      self.out = nn.Softmax(dim=1)\n","    def forward(self, x):\n","      x = self.act(self.bn1(self.fc1(x)))\n","      x = self.act(self.bn2(self.fc2(x)))\n","      x = self.act(self.bn3(self.fc3(x)))\n","      x = self.act(self.bn4(self.fc4(x)))\n","      x = self.out(self.fc5(x))\n","      return x\n","\n","# H = hit\n","# S = stand\n","# D = double\n","\n","# Use GPU\n","model = FNN_BN().to(device)\n","train_data = train_data.to(device)\n","train_labels = train_labels.squeeze().to(device)\n","test_data = test_data.to(device)\n","test_labels = test_labels.squeeze().to(device)\n","\n","# Loss\n","loss_function = nn.CrossEntropyLoss() # This loss function is ideal for the multiple output classifier\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Initialize lists to store metrics\n","train_accuracies = []\n","train_recalls = []\n","train_f1_scores = []\n","\n","# Train model\n","for epoch in range(epochs):\n","    model.train() # Set model to training mode\n","    # Forward pass\n","    outputs = model(train_data)\n","    loss = loss_function(outputs, train_labels)\n","\n","    # Backward and optimize\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    if (epoch + 1) % 1 == 0:\n","      model.eval()\n","      with torch.no_grad():\n","        outputs = model(test_data)\n","        _, predicted = torch.max(outputs.data, 1)\n","\n","        # Convert tensors to numpy arrays for scikit-learn\n","        predicted_np = predicted.cpu().numpy()\n","        test_labels_np = test_labels.cpu().numpy()\n","\n","        # Calculate metrics\n","        accuracy = accuracy_score(test_labels_np, predicted_np)\n","        recall = recall_score(test_labels_np, predicted_np, average='weighted') # Use 'weighted' for multi-class\n","        f1 = f1_score(test_labels_np, predicted_np, average='weighted')       # Use 'weighted' for multi-class\n","\n","        # Store metrics\n","        train_accuracies.append(accuracy)\n","        train_recalls.append(recall)\n","        train_f1_scores.append(f1)\n","\n","        #print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}')\n","\n","import matplotlib.pyplot as plt\n","\n","# Create a list of epoch numbers corresponding to the stored metrics\n","epochs_for_plot = [(i + 1) for i in range(len(train_accuracies))]\n","\n","# Plot the metrics\n","plt.figure(figsize=(10, 6))\n","plt.plot(epochs_for_plot, train_accuracies, label='Accuracy')\n","plt.plot(epochs_for_plot, train_recalls, label='Recall')\n","plt.plot(epochs_for_plot, train_f1_scores, label='F1-Score')\n","\n","plt.xlabel('Epoch')\n","plt.ylabel('Metric Value')\n","plt.title('FNN Training Metrics Over Epochs')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","file_path_model = '/content/drive/MyDrive/Classes/Intro ML/Project/Models/FNN_BN_2.pth'\n","torch.save(model, file_path_model)"],"metadata":{"id":"CVuKrUVS_hh-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Metrics for FNN with batch normalization (Test Data):\")\n","print(\"Accuracy: \\t\", train_accuracies[-1])\n","print(\"Recall: \\t\", train_recalls[-1])\n","print(\"F1: \\t\\t\", train_f1_scores[-1])\n","\n","# Calculate metrics for the training dataset\n","model.eval() # Set the model to evaluation mode\n","with torch.no_grad():\n","    train_outputs = model(train_data)\n","    _, train_predicted = torch.max(train_outputs.data, 1)\n","\n","    # Convert tensors to numpy arrays for scikit-learn\n","    train_predicted_np = train_predicted.cpu().numpy()\n","    train_labels_np = train_labels.cpu().numpy()\n","\n","    # Calculate metrics for training data\n","    train_accuracy = accuracy_score(train_labels_np, train_predicted_np)\n","    train_recall = recall_score(train_labels_np, train_predicted_np, average='weighted')\n","    train_f1 = f1_score(train_labels_np, train_predicted_np, average='weighted')\n","\n","print(\"\\nMetrics for FNN with batch normalization (Train Data):\")\n","print(\"Accuracy: \\t\", train_accuracy)\n","print(\"Recall: \\t\", train_recall)\n","print(\"F1: \\t\\t\", train_f1)"],"metadata":{"id":"7pQDmSqg44lw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# FNN with Dropout & Weight Decay\n"],"metadata":{"id":"iJ5PMvUKAKKz"}},{"cell_type":"code","source":["import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from sklearn.metrics import accuracy_score, recall_score, f1_score # Import scikit-learn metrics\n","\n","# Get GPU\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(f'Using device: {device}')\n","\n","# Training parameters\n","epochs = 300\n","learning_rate = 1e-2\n","\n","# Model\n","input_features = 4   # Training features\n","output_features = 3                   # Number of actions\n","hidden_layers = [10, 20, 30, 20]\n","class FNN_D_WD(nn.Module):\n","    def __init__(self):\n","      super(FNN_D_WD, self).__init__()\n","      self.act = nn.ReLU()\n","      self.fc1 = nn.Linear(input_features, hidden_layers[0])\n","      self.fc2 = nn.Linear(hidden_layers[0], hidden_layers[1])\n","      self.fc3 = nn.Linear(hidden_layers[1], hidden_layers[2])\n","      self.fc4 = nn.Linear(hidden_layers[2], hidden_layers[3])\n","      self.fc5 = nn.Linear(hidden_layers[3], output_features)\n","      self.reg = nn.Dropout(p=0.5)\n","      self.out = nn.Softmax(dim=1)\n","    def forward(self, x):\n","      x = self.act(self.reg(self.fc1(x)))\n","      x = self.act(self.reg(self.fc2(x)))\n","      x = self.act(self.reg(self.fc3(x)))\n","      x = self.act(self.reg(self.fc4(x)))\n","      x = self.out(self.fc5(x))\n","      return x\n","\n","# H = hit\n","# S = stand\n","# D = double\n","\n","# Use GPU\n","model = FNN_D_WD().to(device)\n","train_data = train_data.to(device)\n","train_labels = train_labels.squeeze().to(device)\n","test_data = test_data.to(device)\n","test_labels = test_labels.squeeze().to(device)\n","\n","# Loss\n","loss_function = nn.CrossEntropyLoss() # This loss function is ideal for the multiple output classifier\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.001)\n","\n","# Initialize lists to store metrics\n","train_accuracies = []\n","train_recalls = []\n","train_f1_scores = []\n","\n","# Train model\n","for epoch in range(epochs):\n","    # Forward pass\n","    outputs = model(train_data)\n","    loss = loss_function(outputs, train_labels)\n","\n","    # Backward and optimize\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    if (epoch + 1) % 1 == 0:\n","      model.eval()\n","      with torch.no_grad():\n","        outputs = model(test_data)\n","        _, predicted = torch.max(outputs.data, 1)\n","\n","        # Convert tensors to numpy arrays for scikit-learn\n","        predicted_np = predicted.cpu().numpy()\n","        test_labels_np = test_labels.cpu().numpy()\n","\n","        # Calculate metrics\n","        accuracy = accuracy_score(test_labels_np, predicted_np)\n","        recall = recall_score(test_labels_np, predicted_np, average='weighted') # Use 'weighted' for multi-class\n","        f1 = f1_score(test_labels_np, predicted_np, average='weighted')       # Use 'weighted' for multi-class\n","\n","        # Store metrics\n","        train_accuracies.append(accuracy)\n","        train_recalls.append(recall)\n","        train_f1_scores.append(f1)\n","\n","        #print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}')\n","\n","import matplotlib.pyplot as plt\n","\n","# Create a list of epoch numbers corresponding to the stored metrics\n","epochs_for_plot = [(i + 1) for i in range(len(train_accuracies))]\n","\n","# Plot the metrics\n","plt.figure(figsize=(10, 6))\n","plt.plot(epochs_for_plot, train_accuracies, label='Accuracy')\n","plt.plot(epochs_for_plot, train_recalls, label='Recall')\n","plt.plot(epochs_for_plot, train_f1_scores, label='F1-Score')\n","\n","plt.xlabel('Epoch')\n","plt.ylabel('Metric Value')\n","plt.title('FNN Training Metrics Over Epochs')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","file_path_model = '/content/drive/MyDrive/Classes/Intro ML/Project/Models/FNN_D_WD_2.pth'\n","torch.save(model, file_path_model)"],"metadata":{"id":"WIcGKzijAOHv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Metrics for FNN with dropout and weight decay (Test Data):\")\n","print(\"Accuracy: \\t\", train_accuracies[-1])\n","print(\"Recall: \\t\", train_recalls[-1])\n","print(\"F1: \\t\\t\", train_f1_scores[-1])\n","\n","# Calculate metrics for the training dataset\n","model.eval() # Set the model to evaluation mode\n","with torch.no_grad():\n","    train_outputs = model(train_data)\n","    _, train_predicted = torch.max(train_outputs.data, 1)\n","\n","    # Convert tensors to numpy arrays for scikit-learn\n","    train_predicted_np = train_predicted.cpu().numpy()\n","    train_labels_np = train_labels.cpu().numpy()\n","\n","    # Calculate metrics for training data\n","    train_accuracy = accuracy_score(train_labels_np, train_predicted_np)\n","    train_recall = recall_score(train_labels_np, train_predicted_np, average='weighted')\n","    train_f1 = f1_score(train_labels_np, train_predicted_np, average='weighted')\n","\n","print(\"\\nMetrics for FNN with dropout and weight decay (Train Data):\")\n","print(\"Accuracy: \\t\", train_accuracy)\n","print(\"Recall: \\t\", train_recall)\n","print(\"F1: \\t\\t\", train_f1)"],"metadata":{"id":"YOK_XqOp4Vw4"},"execution_count":null,"outputs":[]}]}