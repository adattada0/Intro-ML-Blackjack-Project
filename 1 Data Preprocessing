{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["mdEm5b-8OyN2","1BHkDgqAO2f1","11u4--ssTVgW"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Import Data"],"metadata":{"id":"mdEm5b-8OyN2"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":339},"id":"1AnYBl_HU5Vv","executionInfo":{"status":"error","timestamp":1765252654414,"user_tz":300,"elapsed":22757,"user":{"displayName":"Eli Rinaldo","userId":"05791135664548871460"}},"outputId":"7dc42d89-de4d-40e3-a0b1-74c350e5e2fd","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/Classes/Intro ML/Project/Dataset/data.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1028470504.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfile_path_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Classes/Intro ML/Project/Dataset/data.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Classes/Intro ML/Project/Dataset/data.csv'"]}],"source":["from google.colab import drive\n","import pandas as pd\n","\n","# Import data\n","drive.mount('/content/drive', force_remount=True)\n","file_path_raw = '/content/drive/MyDrive/Classes/Intro ML/Project/Dataset/data.csv'\n","data = pd.read_csv(file_path_raw)"]},{"cell_type":"code","source":["import gc\n","\n","random_sample_df = data\n","\n","del data\n","gc.collect()"],"metadata":{"id":"dKwN90YCW1rE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Clean and Process Data"],"metadata":{"id":"1BHkDgqAO2f1"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import ast\n","\n","# --- 1. Clean 'actions_taken' into a true list of actions ---\n","def clean_actions(actions_str):\n","    \"\"\"Safely converts the string representation of actions into a Python list of lists.\"\"\"\n","    if pd.isna(actions_str):\n","        return np.nan\n","    try:\n","        # ast.literal_eval handles the string format like \"[['P', 'H', 'S'], ['H', 'S']]\"\n","        return ast.literal_eval(actions_str)\n","    except (ValueError, TypeError, SyntaxError):\n","        # Handle cases where the format is unexpected\n","        return np.nan\n","\n","# Apply the cleaning function to create a new column containing the clean list of lists\n","random_sample_df['actions_list'] = random_sample_df['actions_taken'].apply(clean_actions)\n","\n","# --- 2. Create the 'is_split' flag ---\n","def check_for_split(actions_list):\n","    \"\"\"Checks if the list of actions contains the 'P' (Split) action.\"\"\"\n","    if not isinstance(actions_list, list):\n","        return 0 # Not a list, so not a split hand\n","\n","    # Check ALL sub-lists for the 'P' action\n","    for sequence in actions_list:\n","        if isinstance(sequence, list):\n","            # Convert actions to uppercase and check for 'P'\n","            if 'P' in [a.upper() for a in sequence]:\n","                return 1\n","    return 0\n","\n","# Apply the check to identify rows that were split\n","random_sample_df['is_split'] = random_sample_df['actions_list'].apply(check_for_split).astype(int)\n","\n","# --- 3. Split the DataFrame ---\n","df_splits = random_sample_df[random_sample_df['is_split'] == 1].copy()\n","df_no_splits = random_sample_df[random_sample_df['is_split'] == 0].copy()\n","\n","print(\" DataFrame successfully split into two subsets.\")\n","print(f\"Total hands: {len(random_sample_df)}\")\n","print(f\"Rows identified as split (df_splits): {len(df_splits)}\")\n","print(f\"Rows identified as non-split (df_no_splits): {len(df_no_splits)}\")"],"metadata":{"id":"JfPdSVMGVyfQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import ast\n","import pandas as pd\n","\n","# --- 1. Helper Function for Aggressive String Cleaning ---\n","\n","def force_clean_string(s):\n","    \"\"\"Applies aggressive string replacements to prepare raw data for ast.literal_eval.\"\"\"\n","    if pd.isna(s):\n","        return s\n","    s = str(s).strip()\n","\n","    # Standardize quotes: Use double quotes for all internal strings\n","    s = s.replace(\"'\", '\"')\n","\n","    # Force quoting around 'BJ' (Blackjack)\n","    s = s.replace('BJ', '\"BJ\"')\n","    s = s.replace('bj', '\"BJ\"') # Account for lowercase\n","\n","    # Attempt to quote common action letters if they appear unquoted (fragile but necessary)\n","    s = s.replace('[H', '[\"H')\n","    s = s.replace(',H', ',\"H')\n","    s = s.replace('H]', 'H\"]')\n","    s = s.replace('[S', '[\"S')\n","    s = s.replace(',S', ',\"S')\n","    s = s.replace('S]', 'S\"]')\n","    s = s.replace('[P', '[\"P')\n","    s = s.replace(',P', ',\"P')\n","    s = s.replace('P]', 'P\"]')\n","    s = s.replace('[D', '[\"D')\n","    s = s.replace(',D', ',\"D')\n","    s = s.replace('D]', 'D\"]')\n","\n","    return s\n","\n","# --- 2. Cleaning Functions ---\n","\n","def clean_split_values(value):\n","    \"\"\"Converts player_final_value (e.g., '[[18], [21]]') to a clean list of ints.\"\"\"\n","    s = force_clean_string(value)\n","    if pd.isna(s): return np.nan\n","\n","    try:\n","        parsed_data = ast.literal_eval(s)\n","        final_values = []\n","\n","        # Logic to flatten and clean:\n","        for item in parsed_data:\n","            # Handle list item ([18] or ['BJ'])\n","            if isinstance(item, list) and item:\n","                val = item[0]\n","            # Handle if the item was accidentally flattened (e.g., 18)\n","            elif isinstance(item, (int, str)):\n","                val = item\n","            else:\n","                continue\n","\n","            if val == 'BJ':\n","                final_values.append(21)\n","            elif isinstance(val, int) or (isinstance(val, str) and val.isdigit()):\n","                val = int(val)\n","                final_values.append(val if val <= 21 else 0)\n","\n","        return final_values if final_values else np.nan\n","    except:\n","        return np.nan\n","\n","def clean_split_actions(actions_str):\n","    \"\"\"Converts actions_taken (e.g., [['P', 'H', 'S'], ['H', 'S']]) to a clean list of lists.\"\"\"\n","    s = force_clean_string(actions_str)\n","    if pd.isna(s): return np.nan\n","\n","    try:\n","        parsed_data = ast.literal_eval(s)\n","\n","        # Check if the result is a list of lists (which indicates split hands)\n","        if isinstance(parsed_data, list) and all(isinstance(i, list) for i in parsed_data):\n","            return parsed_data\n","\n","        # Handle the non-split case that might result in just one list (e.g., ['H', 'S'])\n","        if isinstance(parsed_data, list) and all(isinstance(i, str) for i in parsed_data):\n","            return [parsed_data] # Wrap it in a list to match the split format\n","\n","        return np.nan\n","    except:\n","        return np.nan\n","\n","def clean_split_final_cards(cards_str):\n","    \"\"\"Converts player_final (e.g., '[[10, 8], [A, 9]]') to a clean list of card sequences.\"\"\"\n","    s = force_clean_string(cards_str)\n","    if pd.isna(s): return np.nan\n","\n","    try:\n","        parsed_data = ast.literal_eval(s)\n","\n","        # Check for list of lists\n","        if isinstance(parsed_data, list) and all(isinstance(i, list) for i in parsed_data):\n","            return parsed_data\n","\n","        # Handle non-split case\n","        if isinstance(parsed_data, list) and all(isinstance(i, (int, str)) for i in parsed_data):\n","            return [parsed_data]\n","\n","        return np.nan\n","    except:\n","        return np.nan\n","\n","# --- 3. Apply Cleaning to df_splits ---\n","\n","df_splits['values_list'] = df_splits['player_final_value'].apply(clean_split_values)\n","df_splits['actions_list_sequences'] = df_splits['actions_taken'].apply(clean_split_actions)\n","df_splits['cards_list_sequences'] = df_splits['player_final'].apply(clean_split_final_cards)\n","\n","print(\" Cleaning attempt applied. Checking head for results:\")\n","print(df_splits[['values_list', 'actions_list_sequences', 'cards_list_sequences']].head(10))\n","\n","# --- 4. Validation Check ---\n","# Check a few rows to ensure the lists are the same length\n","print(\"\\nAlignment Check (Sample of list lengths):\")\n","df_check = df_splits[df_splits['values_list'].apply(lambda x: isinstance(x, list))]\n","df_check['values_len'] = df_check['values_list'].apply(len)\n","df_check['actions_len'] = df_check['actions_list_sequences'].apply(lambda x: len(x) if isinstance(x, list) else np.nan)\n","df_check['cards_len'] = df_check['cards_list_sequences'].apply(lambda x: len(x) if isinstance(x, list) else np.nan)\n","\n","print(df_check[['values_len', 'actions_len', 'cards_len']].head(10))"],"metadata":{"id":"B8TAqaAvXG4s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_splits['split_index'] = df_splits['values_list'].apply(\n","    lambda x: list(range(len(x))) if isinstance(x, list) else []\n",")\n","\n","df_splits['split_count'] = df_splits['values_list'].apply(\n","    lambda x: len(x) if isinstance(x, list) and x else np.nan\n",")\n","\n","# Scale the 'win' value: Divide the original win by the number of split outcomes\n","df_splits['win'] = df_splits['win'] / df_splits['split_count']\n","df_splits.drop(columns=['split_count'], inplace=True, errors='ignore')\n","# --- 2. Perform the Explosion ---\n","# Explode both the primary list and the index list simultaneously\n","# Note: df_splits_exploded will now have duplicate rows for all non-exploded columns\n","df_splits_exploded = df_splits.explode(['values_list', 'split_index']).copy()\n","\n","# --- 3. Align the Remaining Columns ---\n","def align_exploded_list(row, column_name):\n","    \"\"\"\n","    Extracts the element from the list-column corresponding to the split_index.\n","    This works because 'actions_list_sequences' and 'cards_list_sequences'\n","    were *not* exploded, but 'split_index' was.\n","    \"\"\"\n","    list_data = row[column_name]\n","    # Ensure index is an integer (explode output) and not NaN\n","    index = row['split_index']\n","\n","    if pd.isna(index) or not isinstance(list_data, list):\n","        return np.nan\n","\n","    # Check if the list exists and the index is valid\n","    index = int(index)\n","    if index < len(list_data):\n","        return list_data[index]\n","\n","    return np.nan\n","\n","# Apply the alignment function to the remaining list columns\n","df_splits_exploded['clean_actions_sequence'] = df_splits_exploded.apply(\n","    lambda row: align_exploded_list(row, 'actions_list_sequences'), axis=1\n",")\n","df_splits_exploded['clean_player_final_cards'] = df_splits_exploded.apply(\n","    lambda row: align_exploded_list(row, 'cards_list_sequences'), axis=1\n",")\n","\n","# --- 4. Final Cleanup of Exploded Data ---\n","df_splits_exploded.rename(columns={'values_list': 'clean_player_value'}, inplace=True)\n","\n","# Drop the temporary and now redundant list columns\n","df_splits_exploded.drop(\n","    columns=['split_index', 'actions_list_sequences', 'cards_list_sequences',\n","             'player_final_value', 'actions_taken', 'player_final'],\n","    inplace=True,\n","    errors='ignore'\n",")\n","\n","print(f\"Total rows in df_splits_exploded: {len(df_splits_exploded)}\")"],"metadata":{"id":"BVNlroc0XLc8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import ast\n","\n","# --- 1. Define Simple Cleaning Functions for Single Hands (REVISED) ---\n","\n","def clean_single_hand_value(value):\n","    \"\"\"Cleans a single hand value (e.g., '[[18]]' or '[['BJ']]') to a single integer.\"\"\"\n","    if pd.isna(value): return np.nan\n","    try:\n","        # Evaluate the string: '[[18]]' -> [[18]]\n","        parsed_data = ast.literal_eval(value)\n","\n","        # UNPACKING STEP: Extract the inner list: [[18]] -> [18]\n","        inner_list = parsed_data[0]\n","\n","        # Extract the value: [18] -> 18\n","        val = inner_list[0]\n","\n","        if val == 'BJ': return 21\n","        # Added safety check for int\n","        if isinstance(val, int): return val if val <= 21 else 0 # 0 for bust\n","        return np.nan\n","    except:\n","        # Added a fallback if the structure is only one level deep\n","        try:\n","            val = ast.literal_eval(value)[0]\n","            if val == 'BJ': return 21\n","            if isinstance(val, int): return val if val <= 21 else 0\n","        except:\n","            return np.nan\n","\n","def clean_single_final_cards(cards_str):\n","    \"\"\"Converts player_final (e.g., '[[10, 7]]') to a clean single list of cards.\"\"\"\n","    if pd.isna(cards_str): return np.nan\n","    try:\n","        # Evaluate the string: '[[10, 7]]' -> [[10, 7]]\n","        parsed_data = ast.literal_eval(cards_str)\n","        # UNPACKING STEP: Return the inner list: [[10, 7]] -> [10, 7]\n","        return parsed_data[0]\n","    except:\n","        # Added a fallback to return the parsed list if it only has one set of brackets\n","        try:\n","            return ast.literal_eval(cards_str)\n","        except:\n","            return np.nan\n","\n","# --- 2. Apply Cleaning to df_no_splits (REVISED clean_actions_sequence) ---\n","\n","print(\"Starting cleanup for df_no_splits...\")\n","\n","# 1. clean_player_value: Now correctly handles the double brackets on the raw column\n","df_no_splits['clean_player_value'] = df_no_splits['player_final_value'].apply(clean_single_hand_value)\n","df_no_splits['clean_player_value'] = df_no_splits['clean_player_value'].fillna(-1).astype(int)\n","# 2. clean_actions_sequence: Apply UNPACKING to the 'actions_list' column\n","# Assuming df_no_splits['actions_list'] contains something like [['H', 'S']]\n","df_no_splits['clean_actions_sequence'] = df_no_splits['actions_list'].apply(lambda x: x[0] if isinstance(x, list) and len(x) == 1 and isinstance(x[0], list) else x)\n","\n","# 3. clean_player_final_cards: Now correctly handles the double brackets on the raw column\n","df_no_splits['clean_player_final_cards'] = df_no_splits['player_final'].apply(clean_single_final_cards)\n","\n","# --- 3. Consolidation: Drop Raw Columns and Align ---\n","# (The consolidation logic remains the same)\n","\n","COLUMNS_TO_DROP_RAW = [\n","    'initial_hand', 'dealer_final', 'dealer_final_value', 'player_final',\n","    'player_final_value', 'actions_taken', 'actions_list'\n","]\n","\n","# Assuming df_splits_exploded and df_no_splits variables are available.\n","# A. Drop raw columns from both DataFrames\n","df_splits_final = df_splits_exploded.drop(columns=COLUMNS_TO_DROP_RAW, errors='ignore')\n","df_no_splits_final = df_no_splits.drop(columns=COLUMNS_TO_DROP_RAW, errors='ignore')\n","\n","# B. Filter columns to ensure they are identical before combining\n","common_cols = list(df_splits_exploded.columns)\n","# Filter the non-split frame to ensure it matches the final column order and set\n","df_no_splits_final = df_no_splits[common_cols]\n","\n","# C. Concatenate the two DataFrames\n","df_final_clean = pd.concat([df_splits_exploded, df_no_splits_final], ignore_index=True)\n","\n","print(f\"Total Rows in Final Dataset: {len(df_final_clean)}\")\n","print(f\"Final Columns: {list(df_final_clean.columns)}\")"],"metadata":{"id":"DL5NbzKuXOO1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. Define a function to safely evaluate and sum the list\n","def calculate_initial_sum(hand_list):\n","    \"\"\"Safely calculates the sum of the cards in the initial hand list.\"\"\"\n","    if pd.isna(hand_list):\n","        return np.nan\n","\n","    # Handle the case where the column is still a string representation of a list (e.g., '[9, 9]')\n","    if isinstance(hand_list, str):\n","        try:\n","\n","            card_values = ast.literal_eval(hand_list)\n","        except:\n","            return np.nan\n","    elif isinstance(hand_list, list):\n","        card_values = hand_list\n","    else:\n","        return np.nan\n","\n","    # Sum the card values\n","    return sum(card_values)\n","\n","# 2. Apply the function to create the new column\n","df_final_clean['initial_hand_value'] = df_final_clean['initial_hand'].apply(calculate_initial_sum)\n","\n","print(\"New column 'initial_hand_value' added successfully.\")\n","print(df_final_clean[['initial_hand', 'initial_hand_value']].head())\n"],"metadata":{"id":"kyOdhhsCXRSJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. Ensure the column is numeric\n","df_final_clean['dealer_final_value'] = pd.to_numeric(df_final_clean['dealer_final_value'], errors='coerce').fillna(0).astype(int)\n","\n","# 2. Create a clean hand value (0 if Bust)\n","df_final_clean['clean_dealer_value'] = df_final_clean['dealer_final_value'].apply(lambda x: x if x <= 21 else 0)\n","df_final_clean = df_final_clean.drop(columns=COLUMNS_TO_DROP_RAW, errors='ignore')"],"metadata":{"id":"0cbvEYt9XT-k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","# 1. Define the four actions\n","ACTIONS = ['H', 'S', 'D', 'P']\n","\n","# 2. Define a function to safely count occurrences\n","def count_action(sequence_list, action_char):\n","    \"\"\"Counts how many times a specific action character appears in the list.\"\"\"\n","    # Ensure the input is a list before attempting to count\n","    if isinstance(sequence_list, list):\n","        return sequence_list.count(action_char)\n","    return 0\n","\n","# 3. Apply the counting function to create the new columns for each action\n","for action in ACTIONS:\n","    col_name = f'count_action_{action}'\n","\n","    df_final_clean[col_name] = df_final_clean['clean_actions_sequence'].apply(\n","        lambda x: count_action(x, action)\n","    )\n","\n","print(df_final_clean[['clean_actions_sequence', 'count_action_H', 'count_action_S', 'count_action_D', 'count_action_P']].head())"],"metadata":{"id":"wUcvbVAgXVvy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Relabel data using simplified blackjack strategy"],"metadata":{"id":"11u4--ssTVgW"}},{"cell_type":"code","source":["def simplified_blackjack_strategy(row):\n","    \"\"\"\n","    creates labels for dataset corresponding to the hard totals chart available on this website\n","    https://www.blackjackapprenticeship.com/blackjack-strategy-charts/\n","    \"\"\"\n","    initial_hand_value = row['initial_hand_value']\n","    dealer_up = row['dealer_up']\n","\n","    if (initial_hand_value >= 17):\n","      return [0, 1, 0]\n","    elif ((initial_hand_value >= 13) & (dealer_up <= 6)):\n","      return [0, 1, 0]\n","    elif ((initial_hand_value >= 13) & (dealer_up >= 7)):\n","      return [1, 0, 0]\n","    elif ((initial_hand_value == 12) & (dealer_up <= 3)):\n","      return [1, 0, 0]\n","    elif ((initial_hand_value == 12) & (dealer_up >= 7)):\n","      return [1, 0, 0]\n","    elif (initial_hand_value == 12):\n","      return [0, 1, 0]\n","    elif (initial_hand_value == 11):\n","      return [0, 0, 1]\n","    elif ((initial_hand_value == 10) & (dealer_up <= 9)):\n","      return [0, 0, 1]\n","    elif (initial_hand_value == 10):\n","      return [1, 0, 0]\n","    elif ((initial_hand_value == 9) & (dealer_up <= 2)):\n","      return [1, 0, 0]\n","    elif ((initial_hand_value == 9) & (dealer_up >= 7)):\n","      return [1, 0, 0]\n","    elif (initial_hand_value == 9):\n","      return [0, 0, 1]\n","    elif (initial_hand_value <= 8):\n","      return [1, 0, 0]\n","    else:\n","      return [0, 1, 0]"],"metadata":{"collapsed":true,"id":"1x0R24_xUPXM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_final_clean['strategy_label'] = df_final_clean.apply(simplified_blackjack_strategy, axis=1)\n","\n","# Split the strategy_label column into three new columns\n","strategy_df = df_final_clean['strategy_label'].apply(pd.Series)\n","strategy_df.columns = ['strategy_hit', 'strategy_stand', 'strategy_double']\n","\n","# Concatenate the new columns with the original DataFrame and drop the original list column\n","df_final_clean = pd.concat([df_final_clean, strategy_df], axis=1)\n","df_final_clean.drop(columns=['strategy_label'], inplace=True)\n","\n","# Display the first few rows with the new columns\n","print(df_final_clean.head())"],"metadata":{"id":"rYdUTVxzf5ah"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Curate processed dataset"],"metadata":{"id":"CTaNcIGCPGQ4"}},{"cell_type":"code","source":["# Select only necessary columns of data\n","train_columns = ['dealer_up', 'initial_hand_value', 'is_split', 'run_count']\n","\n","# Label set 1\n","test_columns_1 = ['count_action_H', 'count_action_S', 'count_action_D', 'win']\n","data = df_final_clean[train_columns + test_columns_1]\n","# Remove rows where 'win' is zero because they do not contain useful info\n","data = data[data['win'] != 0]\n","df_labels_1 = data[['count_action_H', 'count_action_S', 'count_action_D']].mul(data['win'], axis=0)\n","df_data_1 = data[train_columns]\n","#df_1 = pd.concat([df_data, df_labels], axis=1)\n","#print(df_1.head())  # Debug\n","\n","# Label set 2\n","test_columns_2 = ['strategy_hit', 'strategy_stand', 'strategy_double']\n","data = df_final_clean[train_columns + test_columns_2]\n","df_labels_2 = data[test_columns_2]\n","df_data_2 = data[train_columns]\n","#print(df_2.head())  # Debug"],"metadata":{"id":"xW5excsCXYEM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Label set 1\n","\n","# Scaling & Train/Test split\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","import torch\n","\n","# Separate features (X) and target (y) from the original DataFrame before scaling\n","X = df_data_1\n","y = df_labels_1\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Scaling (standardization)\n","scaler = StandardScaler()\n","\n","# Fit the scaler only on the training features and transform both training and test features\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Convert labels to class indices (0, 1, 2, or 3) and to long type for CrossEntropyLoss\n","train_labels_idx = y_train.values.argmax(axis=1)\n","test_labels_idx = y_test.values.argmax(axis=1)\n","\n","# rename\n","train_data_1 = pd.DataFrame(X_train_scaled)\n","train_labels_1 = pd.DataFrame(train_labels_idx)\n","test_data_1 = pd.DataFrame(X_test_scaled)\n","test_labels_1 = pd.DataFrame(test_labels_idx)"],"metadata":{"id":"MZTP6TswXaa4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Label set 2\n","\n","# Scaling & Train/Test split\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","import torch\n","\n","# Separate features (X) and target (y) from the original DataFrame before scaling\n","X = df_data_2\n","y = df_labels_2\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Scaling (standardization)\n","scaler = StandardScaler()\n","\n","# Fit the scaler only on the training features and transform both training and test features\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Convert labels to class indices (0, 1, 2, or 3) and to long type for CrossEntropyLoss\n","train_labels_idx = y_train.values.argmax(axis=1)\n","test_labels_idx = y_test.values.argmax(axis=1)\n","\n","# rename\n","train_data_2 = pd.DataFrame(X_train_scaled)\n","train_labels_2 = pd.DataFrame(train_labels_idx)\n","test_data_2 = pd.DataFrame(X_test_scaled)\n","test_labels_2 = pd.DataFrame(test_labels_idx)"],"metadata":{"id":"VduUF8itiYq1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Write out new CSVs for label set 1\n","file_path_trd_1 = '/content/drive/MyDrive/Classes/Intro ML/Project/Dataset/train_data_1.csv'\n","file_path_trl_1 = '/content/drive/MyDrive/Classes/Intro ML/Project/Dataset/train_labels_1.csv'\n","file_path_ted_1 = '/content/drive/MyDrive/Classes/Intro ML/Project/Dataset/test_data_1.csv'\n","file_path_tel_1 = '/content/drive/MyDrive/Classes/Intro ML/Project/Dataset/test_labels_1.csv'\n","train_data_1.to_csv(file_path_trd_1, index=False)\n","train_labels_1.to_csv(file_path_trl_1, index=False)\n","test_data_1.to_csv(file_path_ted_1, index=False)\n","test_labels_1.to_csv(file_path_tel_1, index=False)\n","\n","# Write out new CSVs for label set 2\n","file_path_trd_2 = '/content/drive/MyDrive/Classes/Intro ML/Project/Dataset/train_data_2.csv'\n","file_path_trl_2 = '/content/drive/MyDrive/Classes/Intro ML/Project/Dataset/train_labels_2.csv'\n","file_path_ted_2 = '/content/drive/MyDrive/Classes/Intro ML/Project/Dataset/test_data_2.csv'\n","file_path_tel_2 = '/content/drive/MyDrive/Classes/Intro ML/Project/Dataset/test_labels_2.csv'\n","train_data_2.to_csv(file_path_trd_2, index=False)\n","train_labels_2.to_csv(file_path_trl_2, index=False)\n","test_data_2.to_csv(file_path_ted_2, index=False)\n","test_labels_2.to_csv(file_path_tel_2, index=False)"],"metadata":{"id":"XrXDzWz1VdAW"},"execution_count":null,"outputs":[]}]}